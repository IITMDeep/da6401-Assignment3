{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11850506,"sourceType":"datasetVersion","datasetId":7446208},{"sourceId":11887389,"sourceType":"datasetVersion","datasetId":7471540}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport os\nimport wandb\nfrom tqdm import tqdm\nimport re\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:16:57.888067Z","iopub.execute_input":"2025-05-21T12:16:57.888588Z","iopub.status.idle":"2025-05-21T12:17:04.593149Z","shell.execute_reply.started":"2025-05-21T12:16:57.888565Z","shell.execute_reply":"2025-05-21T12:17:04.592575Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\n\nos.environ['WANDB_API_KEY'] = secret_value_0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:10.392976Z","iopub.execute_input":"2025-05-21T12:17:10.393834Z","iopub.status.idle":"2025-05-21T12:17:10.515260Z","shell.execute_reply.started":"2025-05-21T12:17:10.393809Z","shell.execute_reply":"2025-05-21T12:17:10.514543Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DEVICE   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBASE_DIR = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:16.402364Z","iopub.execute_input":"2025-05-21T12:17:16.402640Z","iopub.status.idle":"2025-05-21T12:17:16.466466Z","shell.execute_reply.started":"2025-05-21T12:17:16.402618Z","shell.execute_reply":"2025-05-21T12:17:16.465656Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class CharacterEmbedding(nn.Module):\n    # Creating an embedding layer that maps input character indices to embedding vectors.\n    # input_size: number of unique characters (vocabulary size)\n    # embedding_dim: size of each embedding vector\n    def __init__(self, input_size, embedding_dim):\n        super(CharacterEmbedding, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n\n    # Returns corresponding embedding vectors of shape (batch_size, seq_length, embedding_dim)\n    def forward(self, input_seq):\n        # input_seq: a tensor of character indices, typically of shape (batch_size, seq_length)\n        return self.embedding(input_seq)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:17.007439Z","iopub.execute_input":"2025-05-21T12:17:17.008075Z","iopub.status.idle":"2025-05-21T12:17:17.013109Z","shell.execute_reply.started":"2025-05-21T12:17:17.008042Z","shell.execute_reply":"2025-05-21T12:17:17.012478Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# EncoderRNN transforms sequences of token IDs into contextual hidden states\n# Supports GRU, LSTM, or vanilla RNN cells\n# input_size: number of unique tokens\n# hidden_size: size of the RNN hidden state\n# embedding_dim: size of token embedding vectors\n# num_layers: number of stacked recurrent layers\n# cell_type: 'GRU', 'LSTM', or 'RNN'\n# dropout_p: dropout probability between RNN layers (only if num_layers > 1)\n# bidirectional: whether to run the RNN in both forward and backward directions\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, embedding_dim, num_layers=1,cell_type='GRU', dropout_p=0.1, bidirectional=False):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n        self.directions = 2 if bidirectional else 1\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        \n        # Dropout before the RNN (applied to embeddings)\n        self.dropout = nn.Dropout(dropout_p)\n        dropout_p = dropout_p if num_layers > 1 else 0\n        \n        # RNN layer\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, batch_first=True)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, batch_first=True)\n        else:  # Default to RNN\n            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, nonlinearity='tanh', batch_first=True)\n\n    # Forward pass through the encoder\n    def forward(self, input_seq):\n        # Input shape: [batch_size, seq_len]\n        batch_size = input_seq.size(0)\n        \n        # Convert indices to embeddings and apply dropout to embeddings\n        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embedding_dim]\n        embedded = self.dropout(embedded)\n        \n        # Pass through RNN\n        outputs, hidden = self.rnn(embedded)\n        \n        return outputs, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:17.222892Z","iopub.execute_input":"2025-05-21T12:17:17.223123Z","iopub.status.idle":"2025-05-21T12:17:17.231437Z","shell.execute_reply.started":"2025-05-21T12:17:17.223105Z","shell.execute_reply":"2025-05-21T12:17:17.230776Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.energy_layer = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.context_vector = nn.Parameter(torch.empty(hidden_dim))\n        nn.init.uniform_(self.context_vector, -0.1, 0.1)\n\n    def forward(self, decoder_hidden, encoder_output_seq):\n        # decoder_hidden: [batch_size, hidden_dim]\n        # encoder_output_seq: [batch_size, seq_length, hidden_dim]\n\n        batch_sz = encoder_output_seq.size(0)\n        seq_len = encoder_output_seq.size(1)\n\n        # Expand decoder hidden state across time dimension\n        repeated_hidden = decoder_hidden.unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, hidden_dim]\n\n        # Compute attention energies\n        concat_inputs = torch.cat((repeated_hidden, encoder_output_seq), dim=2)  # [batch_size, seq_len, 2*hidden_dim]\n        energy_scores = torch.tanh(self.energy_layer(concat_inputs))            # [batch_size, seq_len, hidden_dim]\n        energy_scores = energy_scores.transpose(1, 2)                            # [batch_size, hidden_dim, seq_len]\n\n        # Prepare context vector for batch multiplication\n        context = self.context_vector.unsqueeze(0).expand(batch_sz, -1).unsqueeze(1)  # [batch_size, 1, hidden_dim]\n\n        # Compute alignment scores\n        alignment = torch.bmm(context, energy_scores).squeeze(1)  # [batch_size, seq_len]\n\n        # Normalize scores into probabilities\n        attention_weights = F.softmax(alignment, dim=1)           # [batch_size, seq_len]\n\n        return attention_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:20.702468Z","iopub.execute_input":"2025-05-21T12:17:20.702740Z","iopub.status.idle":"2025-05-21T12:17:20.708814Z","shell.execute_reply.started":"2025-05-21T12:17:20.702703Z","shell.execute_reply":"2025-05-21T12:17:20.708133Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class DecoderRNNWithAttention(nn.Module):\n    def __init__(self, output_size, hidden_size, embedding_dim, num_layers=1, \n                 cell_type='GRU', dropout_p=0.1):\n        super(DecoderRNNWithAttention, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        \n        # Embedding for decoder input tokens\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        self.embedding_dropout = nn.Dropout(dropout_p)\n\n        adjusted_dropout = dropout_p if num_layers > 1 else 0\n        rnn_input_dim = embedding_dim + hidden_size  # includes attention context\n\n        # Choose RNN type\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(rnn_input_dim, hidden_size, num_layers, \n                              dropout=adjusted_dropout, batch_first=True)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(rnn_input_dim, hidden_size, num_layers, \n                               dropout=adjusted_dropout, batch_first=True)\n        else:\n            self.rnn = nn.RNN(rnn_input_dim, hidden_size, num_layers, \n                              dropout=adjusted_dropout, nonlinearity='tanh', batch_first=True)\n        \n        # Attention module\n        self.attention = Attention(hidden_size)\n        \n        # Output transformation\n        self.output_dropout = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_token, hidden_state, encoder_outputs):\n        # input_token: [batch_size]\n        # hidden_state: hidden (or hidden, cell) for LSTM\n        # encoder_outputs: [batch_size, seq_len, hidden_size]\n\n        bsz = input_token.size(0)\n\n        # Embed the current input token\n        embedded_input = self.embedding(input_token.squeeze(1)).unsqueeze(1)\n        embedded_input = self.embedding_dropout(embedded_input)\n\n        # Select last hidden state (handle LSTM separately)\n        if self.cell_type == 'LSTM':\n            current_hidden = hidden_state[0][-1]\n        else:\n            current_hidden = hidden_state[-1]\n\n        # Compute attention weights\n        attention_scores = self.attention(current_hidden, encoder_outputs)  # [batch_size, seq_len]\n        attention_scores = attention_scores.unsqueeze(1)                    # [batch_size, 1, seq_len]\n\n        # Compute context vector via weighted sum\n        context_vector = torch.bmm(attention_scores, encoder_outputs)       # [batch_size, 1, hidden_size]\n\n        # Concatenate embedding and context for RNN input\n        combined_input = torch.cat((embedded_input, context_vector), dim=2) # [batch_size, 1, input_dim]\n\n        # Run the RNN cell\n        rnn_output, new_hidden_state = self.rnn(combined_input, hidden_state)\n\n        # Generate output\n        rnn_output = self.output_dropout(rnn_output)\n        logits = self.fc(rnn_output.squeeze(1))  # [batch_size, output_size]\n        \n        return F.log_softmax(logits, dim=1), new_hidden_state, attention_scores.squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:20.952468Z","iopub.execute_input":"2025-05-21T12:17:20.952867Z","iopub.status.idle":"2025-05-21T12:17:20.961405Z","shell.execute_reply.started":"2025-05-21T12:17:20.952847Z","shell.execute_reply":"2025-05-21T12:17:20.960639Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def beam_search_decode(model, src, sos_idx, eos_idx, max_len=30, beam_width=3, device='cuda'):\n    model.eval()\n    with torch.no_grad():\n        # Encode source sequence\n        encoder_outputs, encoder_hidden = model.encoder(src)\n\n        # Initialize decoder hidden state\n        if model.bidirectional:\n            if model.cell_type == 'LSTM':\n                h_enc, c_enc = encoder_hidden\n                h_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size, device=device)\n                c_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size, device=device)\n                for i in range(model.encoder.num_layers):\n                    h_comb = torch.cat((h_enc[2*i], h_enc[2*i+1]), dim=1)\n                    c_comb = torch.cat((c_enc[2*i], c_enc[2*i+1]), dim=1)\n                    h_dec[i] = model.hidden_transform(h_comb)\n                    c_dec[i] = model.hidden_transform(c_comb)\n                decoder_hidden = (h_dec, c_dec)\n            else:\n                h_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size, device=device)\n                for i in range(model.encoder.num_layers):\n                    h_comb = torch.cat((encoder_hidden[2*i], encoder_hidden[2*i+1]), dim=1)\n                    h_dec[i] = model.hidden_transform(h_comb)\n                decoder_hidden = h_dec\n        else:\n            decoder_hidden = encoder_hidden\n\n        # Initialize beam search\n        beams = [([sos_idx], 0.0, decoder_hidden)]\n        completed = []\n\n        for _ in range(max_len):\n            candidates = []\n            for seq, score, hidden in beams:\n                if seq[-1] == eos_idx:\n                    completed.append((seq, score))\n                    continue\n\n                inp = torch.tensor([seq[-1]], device=device)\n                output, next_hidden, _ = model.decoder(inp, hidden, encoder_outputs)\n                \n                top_log_probs, top_indices = torch.topk(output.squeeze(0), beam_width)\n                for i in range(beam_width):\n                    next_token = top_indices[i].item()\n                    new_seq = seq + [next_token]\n                    new_score = score + top_log_probs[i].item()\n                    if model.cell_type == 'LSTM':\n                        detached_hidden = tuple(h.detach() for h in next_hidden)\n                        candidates.append((new_seq, new_score, detached_hidden))\n                    else:\n                        candidates.append((new_seq, new_score, next_hidden.detach()))\n\n            # Select top-k beams\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n            if not beams:\n                break\n\n        # Collect finished sequences\n        completed.extend([(seq, score) for seq, score, _ in beams if seq[-1] == eos_idx])\n\n        # Fallback if no completed sequence\n        if not completed:\n            completed = beams\n\n        # Return sorted results\n        completed = sorted(completed, key=lambda x: x[1], reverse=True)\n        return completed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:20.967329Z","iopub.execute_input":"2025-05-21T12:17:20.967847Z","iopub.status.idle":"2025-05-21T12:17:20.989068Z","shell.execute_reply.started":"2025-05-21T12:17:20.967819Z","shell.execute_reply":"2025-05-21T12:17:20.988485Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Seq2Seq implements an Encoder and Decoder for end-to-end sequence-to-sequence modeling\n# input_size: size of source vocabulary\n# output_size: size of target vocabulary\n# embedding_dim: dimension of embeddings in both encoder and decoder\n# hidden_size: size of hidden states in encoder and decoder (must match for vanilla seq2seq)\n# encoder_layers / decoder_layers: number of stacked RNN layers\n# cell_type: 'GRU', 'LSTM', or 'RNN'\n# dropout_p: dropout probability for embeddings and RNN layers\n# bidirectional_encoder: if True, runs encoder bidirectionally and transforms hidden state\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_size, output_size, embedding_dim=256, hidden_size=256,\n                 encoder_layers=1, decoder_layers=1, cell_type='GRU', dropout_p=0.2,\n                 bidirectional_encoder=False):\n        super(Seq2Seq, self).__init__()\n\n        self.bidirectional = bidirectional_encoder\n        self.cell_type = cell_type\n\n        self.encoder = EncoderRNN(input_size, hidden_size, embedding_dim,\n                                  num_layers=encoder_layers, cell_type=cell_type,\n                                  dropout_p=dropout_p, bidirectional=bidirectional_encoder)\n\n        if self.bidirectional:\n            self.hidden_transform = nn.Linear(hidden_size * 2, hidden_size)\n\n        self.decoder = DecoderRNNWithAttention(output_size, hidden_size, embedding_dim,\n                                               num_layers=decoder_layers, cell_type=cell_type,\n                                               dropout_p=dropout_p)\n\n    def _match_decoder_layers(self, hidden, batch_size):\n        current_layers = hidden.size(0)\n        if current_layers > self.decoder.num_layers:\n            return hidden[:self.decoder.num_layers]\n        elif current_layers < self.decoder.num_layers:\n            padding = torch.zeros(self.decoder.num_layers - current_layers, batch_size,\n                                  self.decoder.hidden_size, device=hidden.device)\n            return torch.cat([hidden, padding], dim=0)\n        else:\n            return hidden\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5, return_attention=False):\n        batch_size = src.size(0)\n        target_len = trg.size(1)\n        vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(batch_size, target_len, vocab_size, device=src.device)\n        attentions = [] if return_attention else None\n\n        encoder_outputs, encoder_hidden = self.encoder(src)\n        decoder_hidden = None\n\n        if self.bidirectional:\n            if self.cell_type == 'LSTM':\n                h_n, c_n = encoder_hidden\n                h_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size, device=src.device)\n                c_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size, device=src.device)\n                \n                for i in range(self.decoder.num_layers):\n                    enc_layer = min(i, self.encoder.num_layers - 1)\n                    h_cat = torch.cat((h_n[2 * enc_layer], h_n[2 * enc_layer + 1]), dim=1)\n                    c_cat = torch.cat((c_n[2 * enc_layer], c_n[2 * enc_layer + 1]), dim=1)\n                    h_dec[i] = self.hidden_transform(h_cat)\n                    c_dec[i] = self.hidden_transform(c_cat)\n\n                decoder_hidden = (h_dec, c_dec)\n\n            else:\n                h_n = encoder_hidden\n                h_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size, device=src.device)\n                \n                for i in range(self.decoder.num_layers):\n                    enc_layer = min(i, self.encoder.num_layers - 1)\n                    h_cat = torch.cat((h_n[2 * enc_layer], h_n[2 * enc_layer + 1]), dim=1)\n                    h_dec[i] = self.hidden_transform(h_cat)\n\n                decoder_hidden = h_dec\n        else:\n            if self.cell_type == 'LSTM':\n                h_n, c_n = encoder_hidden\n                decoder_hidden = (\n                    self._match_decoder_layers(h_n, batch_size),\n                    self._match_decoder_layers(c_n, batch_size)\n                )\n            else:\n                decoder_hidden = self._match_decoder_layers(encoder_hidden, batch_size)\n\n        input_token = trg[:, 0].unsqueeze(1)\n\n        for t in range(1, target_len):\n            output, decoder_hidden, attn = self.decoder(input_token, decoder_hidden, encoder_outputs)\n            outputs[:, t, :] = output\n\n            if return_attention:\n                attentions.append(attn.unsqueeze(1))\n\n            use_teacher = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1).unsqueeze(1)\n            input_token = trg[:, t].unsqueeze(1) if use_teacher else top1\n\n        if return_attention:\n            attentions = torch.cat(attentions, dim=1)\n            return outputs, attentions\n\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:21.192419Z","iopub.execute_input":"2025-05-21T12:17:21.192897Z","iopub.status.idle":"2025-05-21T12:17:21.204321Z","shell.execute_reply.started":"2025-05-21T12:17:21.192879Z","shell.execute_reply":"2025-05-21T12:17:21.203701Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class LexiconDataset(Dataset):\n    def __init__(self, filepath, src_vocab=None, tgt_vocab=None, build_vocab=False):\n        self.examples = []\n        with open(filepath, encoding='utf-8') as file:\n            for line in file:\n                items = line.strip().split('\\t')\n                if len(items) < 2:\n                    continue\n                tgt_text, src_text = items[0], items[1]  # Hindi and romanized\n                self.examples.append((src_text, tgt_text))\n\n        if build_vocab:\n            self.src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n            self.tgt_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n\n            for src, tgt in self.examples:\n                for ch in src:\n                    if ch not in self.src_vocab:\n                        self.src_vocab[ch] = len(self.src_vocab)\n                for ch in tgt:\n                    if ch not in self.tgt_vocab:\n                        self.tgt_vocab[ch] = len(self.tgt_vocab)\n        else:\n            assert src_vocab is not None and tgt_vocab is not None, \"Prebuilt vocabularies must be provided.\"\n            self.src_vocab = src_vocab\n            self.tgt_vocab = tgt_vocab\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, index):\n        src, tgt = self.examples[index]\n        src_ids = [self.src_vocab.get(ch, self.src_vocab['<unk>']) for ch in src]\n        tgt_ids = [self.tgt_vocab['<sos>']] + \\\n                  [self.tgt_vocab.get(ch, self.tgt_vocab['<unk>']) for ch in tgt] + \\\n                  [self.tgt_vocab['<eos>']]\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n\n\ndef collate_fn(batch):\n    \"\"\"\n    Collates a batch of examples, padding each sequence to the maximum length in the batch.\n    Returns:\n        padded_src: Tensor of shape (batch_size, max_src_len)\n        padded_tgt: Tensor of shape (batch_size, max_tgt_len)\n    \"\"\"\n    src_seqs, tgt_seqs = zip(*batch)\n    max_src_len = max(len(seq) for seq in src_seqs)\n    max_tgt_len = max(len(seq) for seq in tgt_seqs)\n\n    batch_size = len(batch)\n    padded_src = torch.full((batch_size, max_src_len), 0, dtype=torch.long)\n    padded_tgt = torch.full((batch_size, max_tgt_len), 0, dtype=torch.long)\n\n    for i in range(batch_size):\n        padded_src[i, :len(src_seqs[i])] = src_seqs[i]\n        padded_tgt[i, :len(tgt_seqs[i])] = tgt_seqs[i]\n\n    return padded_src, padded_tgt\n\ndef get_dataloaders(data_dir, batch_size, build_vocab=False):\n    \"\"\"\n    Loads training, validation, and test data loaders.\n    Returns:\n        train_loader, val_loader, test_loader,\n        src_vocab_size, tgt_vocab_size, pad_idx, src_vocab, tgt_vocab\n    \"\"\"\n    train_file = os.path.join(data_dir, 'hi.translit.sampled.train.tsv')\n    val_file   = os.path.join(data_dir, 'hi.translit.sampled.dev.tsv')\n    test_file  = os.path.join(data_dir, 'hi.translit.sampled.test.tsv')\n\n    train_dataset = LexiconDataset(train_file, build_vocab=build_vocab)\n    src_vocab = train_dataset.src_vocab\n    tgt_vocab = train_dataset.tgt_vocab\n\n    val_dataset  = LexiconDataset(val_file, src_vocab, tgt_vocab)\n    test_dataset = LexiconDataset(test_file, src_vocab, tgt_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader  = DataLoader(test_dataset,  batch_size=1,          shuffle=False, collate_fn=collate_fn)\n\n    return (train_loader, val_loader, test_loader,\n            len(src_vocab), len(tgt_vocab), src_vocab['<pad>'],\n            src_vocab, tgt_vocab)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:21.482382Z","iopub.execute_input":"2025-05-21T12:17:21.482802Z","iopub.status.idle":"2025-05-21T12:17:21.493253Z","shell.execute_reply.started":"2025-05-21T12:17:21.482784Z","shell.execute_reply":"2025-05-21T12:17:21.492690Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class EarlyStopper:\n    \"\"\"Stops a run if the monitored metric doesn’t improve for `patience` steps.\"\"\"\n    def __init__(self, patience=5, min_delta=1e-4):\n        self.patience, self.min_delta = patience, min_delta\n        self.counter, self.best = 0, None\n\n    def should_stop(self, current):\n        if self.best is None or current > self.best + self.min_delta:\n            self.best, self.counter = current, 0\n        else:\n            self.counter += 1\n        return self.counter >= self.patience","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:29.372889Z","iopub.execute_input":"2025-05-21T12:17:29.373418Z","iopub.status.idle":"2025-05-21T12:17:29.378000Z","shell.execute_reply.started":"2025-05-21T12:17:29.373397Z","shell.execute_reply":"2025-05-21T12:17:29.377254Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"CHAR2IDX_SRC = {\n    \"<pad>\": 0,\n    \"<sos>\": 1,\n    \"<eos>\": 2,\n    \"<unk>\": 3,\n    **{c: i + 4 for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n}\nIDX2CHAR_SRC = {i: c for c, i in CHAR2IDX_SRC.items()}\n\n# Load data, build vocabs, and create reverse target-char map\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\nIDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}  # Map decoder indices back to Hindi chars\n\n# Model, optimizer, loss, early stopping\n\n# parameters of best model\nbest_model = Seq2Seq(\n    input_size=src_size,\n    output_size=tgt_size,\n    embedding_dim=64,\n    hidden_size=256,\n    encoder_layers=3,\n    decoder_layers=1,\n    cell_type='LSTM',  # or 'GRU' or 'RNN'\n    dropout_p=0.4,\n    bidirectional_encoder=False\n).to(DEVICE)\n\noptimizer = torch.optim.Adam(best_model.parameters(), lr=1e-3)\ncriterion = nn.NLLLoss(ignore_index=pad_idx)\nstopper = EarlyStopper(patience=5)\nbest_val_acc = 0.0\n\n# Training Loop\n\nfor epoch in range(1, 11):\n    best_model.train()\n    total_loss = 0.0\n    for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Training\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        out = best_model(src, tgt, teacher_forcing_ratio=1.0)\n        loss = criterion(out.view(-1, tgt_size), tgt.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # Validation: sequence-level accuracy\n    best_model.eval()\n    correct_seqs, total_seqs = 0, 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n            out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n            preds = out.argmax(dim=2)\n            for pred_seq, true_seq in zip(preds, tgt):\n                # Remove <sos> and padding tokens for comparison\n                pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                if torch.equal(pred_tokens, true_tokens):\n                    correct_seqs += 1\n                total_seqs += 1\n\n    val_acc = correct_seqs / total_seqs\n    print(f\"[Epoch {epoch}] Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Optionally save model checkpoint here\n    elif stopper.should_stop(val_acc):\n        print(\"Early stopping triggered.\")\n        break\n\n# Final Test Evaluation\nbest_model.eval()\ncorrect_seqs, total_seqs = 0, 0\nall_preds, all_trues = [], []\n\nwith torch.no_grad():\n    for src, tgt in tqdm(test_loader, desc=\"Final Test Eval\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n        preds = out.argmax(dim=2)\n        for pred_seq, true_seq in zip(preds, tgt):\n            pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n            true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n            if torch.equal(pred_tokens, true_tokens):\n                correct_seqs += 1\n            total_seqs += 1\n\n            all_preds.append(pred_tokens)\n            all_trues.append(true_tokens)\n\ntest_acc = correct_seqs / total_seqs\nprint(f\"\\n Final Test Accuracy (Exact Word match): {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:17:30.177813Z","iopub.execute_input":"2025-05-21T12:17:30.178042Z","iopub.status.idle":"2025-05-21T12:22:15.663376Z","shell.execute_reply.started":"2025-05-21T12:17:30.178026Z","shell.execute_reply":"2025-05-21T12:22:15.662756Z"}},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Loss: 1454.5247 | Val Acc: 0.0321\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Loss: 699.1988 | Val Acc: 0.2295\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Loss: 439.6851 | Val Acc: 0.3355\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Loss: 344.9575 | Val Acc: 0.3531\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Loss: 299.8262 | Val Acc: 0.3811\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Loss: 271.3750 | Val Acc: 0.3878\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Loss: 250.9414 | Val Acc: 0.4071\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Loss: 235.4939 | Val Acc: 0.4169\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Loss: 224.8430 | Val Acc: 0.3961\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Loss: 213.9978 | Val Acc: 0.4312\n","output_type":"stream"},{"name":"stderr","text":"                                                                     ","output_type":"stream"},{"name":"stdout","text":"\n Final Test Accuracy (Exact Word match): 0.1724\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"romanized_test_words = []\ntest_path = os.path.join(BASE_DIR, 'hi.translit.sampled.test.tsv')\n\n# Read romanized inputs from test file\nwith open(test_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        tgt, src, _ = line.strip().split()\n        romanized_test_words.append(src)\n\nbest_model.eval()\nresults = []\n\nwith torch.no_grad():\n    for i, (src_batch, tgt_batch) in enumerate(test_loader):\n        src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n        output = best_model(src_batch, tgt_batch, teacher_forcing_ratio=0.0)\n        predicted_tokens = output.argmax(dim=2)\n\n        for j in range(src_batch.size(0)):\n            pred_str = ''.join(\n                IDX2CHAR_TGT[token.item()]\n                for token in predicted_tokens[j][1:]  # skip <sos>\n                if token.item() != pad_idx\n            )\n            tgt_str = ''.join(\n                IDX2CHAR_TGT[token.item()]\n                for token in tgt_batch[j][1:]  # skip <sos>\n                if token.item() != pad_idx\n            )\n\n            original_input = romanized_test_words[i * src_batch.size(0) + j]\n\n            results.append({\n                'Input': original_input,\n                'True Hindi': tgt_str,\n                'Predicted Hindi': pred_str\n            })\n\n# Show a random sample of the results\nsampled_results = random.sample(results, min(10, len(results)))\ndf = pd.DataFrame(sampled_results)\nprint(df.to_markdown(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:23:09.013247Z","iopub.execute_input":"2025-05-21T12:23:09.013755Z","iopub.status.idle":"2025-05-21T12:23:34.293356Z","shell.execute_reply.started":"2025-05-21T12:23:09.013703Z","shell.execute_reply":"2025-05-21T12:23:34.292631Z"}},"outputs":[{"name":"stdout","text":"| Input     | True Hindi   | Predicted Hindi   |\n|:----------|:-------------|:------------------|\n| bikherati | बिखेरती<eos>    | बिखेरते<eos>         |\n| supachya  | सुपाच्य<eos>    | सुपच्य<eos><eos>    |\n| rishikul  | ऋषिकुल<eos>    | रिशिकुल              |\n| dabholkar | दाभोलकर<eos>   | दभोलकर<eos><eos>   |\n| ashariri  | अशरीरी<eos>    | अशारीरी              |\n| premlata  | प्रेमलता<eos>   | प्रेमलता<eos>        |\n| hamaal    | हमाल<eos>     | हामाल               |\n| mavey     | मावे<eos>      | मवे<eos><eos>      |\n| kasida    | कसीदा<eos>     | कसीदा<eos>          |\n| forcee    | फारसी<eos>     | फोर्सी               |\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def highlight_pred(row):\n    \"\"\"\n    Returns a list of CSS styles, one per column, \n    coloring the 'Predicted Hindi' cell green if correct else red.\n    \"\"\"\n    styles = [''] * len(row)\n    # Find the index of the Predicted column\n    pred_idx = list(row.index).index('Predicted Hindi')\n    if row['Predicted Hindi'] == row['True Hindi']:\n        styles[pred_idx] = 'background-color: #c8e6c9; font-weight: bold;'  # light green\n    else:\n        styles[pred_idx] = 'background-color: #f8d7da; font-weight: bold;'  # light red\n    return styles\n\n# Apply to a random sample of 10 rows\nsubset = df.sample(n=min(10, len(df))).reset_index(drop=True)\n\nstyled = (\n    subset.style\n          .apply(highlight_pred, axis=1)\n          .set_table_styles([\n              # Center all text\n              {'selector': 'td, th',\n               'props': [('text-align', 'center'), ('padding', '6px')]},\n              # Header style\n              {'selector': 'th',\n               'props': [('background-color', '#4F81BD'),\n                         ('color', 'white'),\n                         ('font-weight', 'bold'),\n                         ('padding', '8px')]}\n          ])\n          .set_caption(\"✨ Sample Transliteration Predictions (Green = Correct, Red = Wrong) ✨\")\n)\n\n# Display in a Jupyter/HTML context\ndisplay(styled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:24:54.066504Z","iopub.execute_input":"2025-05-21T09:24:54.067081Z","iopub.status.idle":"2025-05-21T09:24:54.183532Z","shell.execute_reply.started":"2025-05-21T09:24:54.067056Z","shell.execute_reply":"2025-05-21T09:24:54.182405Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7fd17f455090>","text/html":"<style type=\"text/css\">\n#T_d48f0 td {\n  text-align: center;\n  padding: 6px;\n}\n#T_d48f0  th {\n  text-align: center;\n  padding: 6px;\n}\n#T_d48f0 th {\n  background-color: #4F81BD;\n  color: white;\n  font-weight: bold;\n  padding: 8px;\n}\n#T_d48f0_row0_col2, #T_d48f0_row4_col2, #T_d48f0_row5_col2, #T_d48f0_row7_col2, #T_d48f0_row9_col2 {\n  background-color: #f8d7da;\n  font-weight: bold;\n}\n#T_d48f0_row1_col2, #T_d48f0_row2_col2, #T_d48f0_row3_col2, #T_d48f0_row6_col2, #T_d48f0_row8_col2 {\n  background-color: #c8e6c9;\n  font-weight: bold;\n}\n</style>\n<table id=\"T_d48f0\">\n  <caption>✨ Sample Transliteration Predictions (Green = Correct, Red = Wrong) ✨</caption>\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_d48f0_level0_col0\" class=\"col_heading level0 col0\" >Input</th>\n      <th id=\"T_d48f0_level0_col1\" class=\"col_heading level0 col1\" >True Hindi</th>\n      <th id=\"T_d48f0_level0_col2\" class=\"col_heading level0 col2\" >Predicted Hindi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_d48f0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_d48f0_row0_col0\" class=\"data row0 col0\" >ria</td>\n      <td id=\"T_d48f0_row0_col1\" class=\"data row0 col1\" >रिया<eos></td>\n      <td id=\"T_d48f0_row0_col2\" class=\"data row0 col2\" >रियर<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_d48f0_row1_col0\" class=\"data row1 col0\" >mukartey</td>\n      <td id=\"T_d48f0_row1_col1\" class=\"data row1 col1\" >मुकरते<eos></td>\n      <td id=\"T_d48f0_row1_col2\" class=\"data row1 col2\" >मुकरते<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_d48f0_row2_col0\" class=\"data row2 col0\" >maulviyon</td>\n      <td id=\"T_d48f0_row2_col1\" class=\"data row2 col1\" >मौलवियों<eos></td>\n      <td id=\"T_d48f0_row2_col2\" class=\"data row2 col2\" >मौलवियों<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_d48f0_row3_col0\" class=\"data row3 col0\" >mauley</td>\n      <td id=\"T_d48f0_row3_col1\" class=\"data row3 col1\" >मौले<eos></td>\n      <td id=\"T_d48f0_row3_col2\" class=\"data row3 col2\" >मौले<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_d48f0_row4_col0\" class=\"data row4 col0\" >dreijer</td>\n      <td id=\"T_d48f0_row4_col1\" class=\"data row4 col1\" >ड्रेजर<eos></td>\n      <td id=\"T_d48f0_row4_col2\" class=\"data row4 col2\" >ड़िजरेड</td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_d48f0_row5_col0\" class=\"data row5 col0\" >tapoowo</td>\n      <td id=\"T_d48f0_row5_col1\" class=\"data row5 col1\" >टापुओं<eos></td>\n      <td id=\"T_d48f0_row5_col2\" class=\"data row5 col2\" >पपूवता<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_d48f0_row6_col0\" class=\"data row6 col0\" >dikhaain</td>\n      <td id=\"T_d48f0_row6_col1\" class=\"data row6 col1\" >दिखाईं<eos></td>\n      <td id=\"T_d48f0_row6_col2\" class=\"data row6 col2\" >दिखाईं<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_d48f0_row7_col0\" class=\"data row7 col0\" >ghost</td>\n      <td id=\"T_d48f0_row7_col1\" class=\"data row7 col1\" >घोस्ट<eos></td>\n      <td id=\"T_d48f0_row7_col2\" class=\"data row7 col2\" >घोस्सो</td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_d48f0_row8_col0\" class=\"data row8 col0\" >changul</td>\n      <td id=\"T_d48f0_row8_col1\" class=\"data row8 col1\" >चंगुल<eos></td>\n      <td id=\"T_d48f0_row8_col2\" class=\"data row8 col2\" >चंगुल<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_d48f0_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_d48f0_row9_col0\" class=\"data row9 col0\" >moodi</td>\n      <td id=\"T_d48f0_row9_col1\" class=\"data row9 col1\" >मूडी<eos></td>\n      <td id=\"T_d48f0_row9_col2\" class=\"data row9 col2\" >मूद्द</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"wandb.init(project=\"DL_Assignment_3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:22:27.107980Z","iopub.execute_input":"2025-05-21T12:22:27.108677Z","iopub.status.idle":"2025-05-21T12:22:40.590931Z","shell.execute_reply.started":"2025-05-21T12:22:27.108651Z","shell.execute_reply":"2025-05-21T12:22:40.590203Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m019\u001b[0m (\u001b[33mcs24m019-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_122233-rcv0nmen</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m019-iitm/DL_Assignment_3/runs/rcv0nmen' target=\"_blank\">vital-dust-165</a></strong> to <a href='https://wandb.ai/cs24m019-iitm/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m019-iitm/DL_Assignment_3' target=\"_blank\">https://wandb.ai/cs24m019-iitm/DL_Assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m019-iitm/DL_Assignment_3/runs/rcv0nmen' target=\"_blank\">https://wandb.ai/cs24m019-iitm/DL_Assignment_3/runs/rcv0nmen</a>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs24m019-iitm/DL_Assignment_3/runs/rcv0nmen?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7bf0ba4873d0>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"best_model.eval()\nall_samples = []\n\n# First, collect all model outputs\nwith torch.no_grad():\n    for i, (src, tgt) in enumerate(test_loader):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out, attn_weights = best_model(src, tgt, return_attention=True)\n        batch_size = src.size(0)\n\n        for b in range(batch_size):\n            romanized = romanized_test_words[i * batch_size + b]\n            src_tokens = list(romanized)\n            tgt_tokens = [IDX2CHAR_TGT[idx.item()] for idx in tgt[b][1:] if idx.item() != pad_idx]\n            pred_tokens = out.argmax(dim=2)[b][1:len(tgt_tokens)+1]\n            pred_chars = [IDX2CHAR_TGT[idx.item()] for idx in pred_tokens]\n\n            attn = attn_weights[b][:len(pred_chars), :len(src_tokens)].cpu().numpy().tolist()\n\n            all_samples.append((src_tokens, pred_chars, attn))\n\n# Select 10 random samples\nrandom_samples = random.sample(all_samples, 5)\n\n# Build HTML blocks\nhtml_blocks = []\nfor sample_count, (src_tokens, pred_chars, attn) in enumerate(random_samples):\n    input_tokens_js = json.dumps(src_tokens, ensure_ascii=False)\n    output_tokens_js = json.dumps(pred_chars, ensure_ascii=False)\n    attention_js = json.dumps(attn)\n\n    html_block = f\"\"\"\n    <div style=\"margin-bottom: 50px;\">\n      <h2>Sample {sample_count + 1}</h2>\n      <div><strong>Input (English):</strong></div>\n      <div id=\"input-tokens-{sample_count}\"></div>\n      <div><strong>Predicted Output (Hindi):</strong></div>\n      <div id=\"output-tokens-{sample_count}\"></div>\n      <script>\n        const inputTokens_{sample_count} = {input_tokens_js};\n        const outputTokens_{sample_count} = {output_tokens_js};\n        const attention_{sample_count} = {attention_js};\n\n        const inputDiv_{sample_count} = d3.select(\"#input-tokens-{sample_count}\");\n        const outputDiv_{sample_count} = d3.select(\"#output-tokens-{sample_count}\");\n\n        inputTokens_{sample_count}.forEach((token, i) => {{\n          inputDiv_{sample_count}.append(\"span\")\n            .attr(\"class\", \"token input\")\n            .attr(\"id\", \"input-{sample_count}-\" + i)\n            .text(token);\n        }});\n\n        outputTokens_{sample_count}.forEach((token, i) => {{\n          outputDiv_{sample_count}.append(\"span\")\n            .attr(\"class\", \"token output\")\n            .text(token)\n            .on(\"mouseover\", () => {{\n              d3.selectAll(\".token.input\").style(\"background-color\", \"#fff\");\n              attention_{sample_count}[i].forEach((score, j) => {{\n                const color = d3.interpolateOranges(score);\n                d3.select(\"#input-{sample_count}-\" + j).style(\"background-color\", color);\n              }});\n            }})\n            .on(\"mouseout\", () => {{\n              d3.selectAll(\".token.input\").style(\"background-color\", \"#fff\");\n            }});\n        }});\n      </script>\n    </div>\n    \"\"\"\n    html_blocks.append(html_block)\n\n# Full HTML document\nfull_html = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <title>Attention Visualizations</title>\n  <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n  <style>\n    body {{ font-family: Arial, sans-serif; margin: 30px; }}\n    .token {{\n      display: inline-block;\n      padding: 8px 12px;\n      margin: 3px;\n      border-radius: 5px;\n      border: 1px solid #ccc;\n      font-size: 20px;\n      cursor: pointer;\n      user-select: none;\n      transition: background-color 0.3s;\n    }}\n  </style>\n</head>\n<body>\n  <h1>Random Attention Visualizations (5 Samples)</h1>\n  {''.join(html_blocks)}\n</body>\n</html>\n\"\"\"\n\n# Log to WandB\nwandb.log({\"attention_visualizations_random_10\": wandb.Html(full_html)})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:23:39.193233Z","iopub.execute_input":"2025-05-21T12:23:39.193505Z","iopub.status.idle":"2025-05-21T12:24:05.136594Z","shell.execute_reply.started":"2025-05-21T12:23:39.193486Z","shell.execute_reply":"2025-05-21T12:24:05.135886Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:24:05.137869Z","iopub.execute_input":"2025-05-21T12:24:05.138142Z","iopub.status.idle":"2025-05-21T12:24:05.700394Z","shell.execute_reply.started":"2025-05-21T12:24:05.138117Z","shell.execute_reply":"2025-05-21T12:24:05.699639Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vital-dust-165</strong> at: <a href='https://wandb.ai/cs24m019-iitm/DL_Assignment_3/runs/rcv0nmen' target=\"_blank\">https://wandb.ai/cs24m019-iitm/DL_Assignment_3/runs/rcv0nmen</a><br> View project at: <a href='https://wandb.ai/cs24m019-iitm/DL_Assignment_3' target=\"_blank\">https://wandb.ai/cs24m019-iitm/DL_Assignment_3</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_122233-rcv0nmen/logs</code>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}